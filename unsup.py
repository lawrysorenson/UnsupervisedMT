# -*- coding: utf-8 -*-
"""401R Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ftr030cpKz87Lhc2HKo3KtB43o66NIi9
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install transformers

import sys
import torch
from transformers import EncoderDecoderModel, BertTokenizer
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
#from torchvision import transforms, utils, datasets
from tqdm import tqdm
from torch.nn.parameter import Parameter
import pdb
import gc
import copy
from transformers import BartForConditionalGeneration, BartConfig

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing
from transformers import BartTokenizer


#from google.colab import drive
#drive.mount('/content/gdrive')

path = "cleaning/"

files = [path + file for file in ["Sorenson-withOPUS.en-US", "Sorenson-withOPUS.fa-IR"]]

class TextDataset(Dataset):
  def __init__(self, files):
    self.data = []
    for file in files:
      lang = '['+file[-5:-3].upper()+']'
      print(lang)
      with open(file, 'r') as f:
          self.data.extend([(l.strip(), lang) for l in f.readlines() if len(l) < 200])

  def __getitem__(self, i):
    return self.data[i]
  
  def __len__(self):
    return len(self.data)

train_dataset_eng = TextDataset(files)

train_dataset_loader = DataLoader(train_dataset_eng, batch_size=8, pin_memory=True, shuffle=True)

tokenizer = Tokenizer(BPE(unk_token="[UNK]", cls_token="[CLS]", sep_token="[SEP]", pad_token="[PAD]", mask_token="[MASK]"))


trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]", "[EN]", "[FA]"])

tokenizer.pre_tokenizer = Whitespace()

tokenizer.train(files, trainer)

tokenizer.post_processor = TemplateProcessing(
    single="$A [SEP]",
    special_tokens=[
        #("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)

print('ADDED POST')

def scope():
  gc.collect()

  configuration = BartConfig( vocab_size = tokenizer.get_vocab_size(),
                              max_position_embeddings = 512,
                              encoder_layers = 6,
                              encoder_ffn_dim = 2048,
                              encoder_attention_heads = 8,
                              decoder_layers = 6,
                              decoder_ffn_dim = 2048,
                              decoder_attention_heads = 8,
                              encoder_layerdrop = 0.0,
                              decoder_layerdrop = 0.0,
                              activation_function = 'swish',
                              d_model = 512,
                              dropout = 0.1,
                              attention_dropout = 0.0,
                              activation_dropout = 0.0,
                              init_std = 0.02,
                              classifier_dropout = 0.0,
                              scale_embedding = True,
                              pad_token_id = tokenizer.token_to_id("[PAD]"),
                              bos_token_id = 0,
                              eos_token_id = tokenizer.token_to_id("[CLS]"),
                              is_encoder_decoder = True,
                              decoder_start_token_id = tokenizer.token_to_id("[EN]"),
                              forced_eos_token_id = tokenizer.token_to_id("[CLS]") )


  model = BartForConditionalGeneration(configuration)
  model.cuda()

  objective = nn.CrossEntropyLoss(ignore_index = 3) #(tokenizer.pad_token_id)
  optimizer = optim.Adam(model.parameters(), lr=3e-4)

  def prep_batch(sents):
    ss = []
    ts = []
    for s, k in zip(*sents):
      source = tokenizer.encode(s).ids
      targ = [tokenizer.token_to_id(k)] + source[:-1]
      ss.append(source)
      ts.append(targ)
    
    pad_len = max([len(s) for s in ss])

    for s in ss:
        s.extend([tokenizer.token_to_id("[PAD]")] * (pad_len - len(s)))

    for t in ts:
        t.extend([tokenizer.token_to_id("[PAD]")] * (pad_len - len(t)))
    
    return ss, ts

  for epoch in range(10):
    batch = 0
    loop = tqdm(total=len(train_dataset_loader))
    for sent in train_dataset_loader: # TODO: deal with batching
      batch += 1
      #print('BATCH', batch)
  
      input_enc, input_dec = prep_batch(sent)
  
      input_enc = torch.tensor(input_enc)
      input_dec = torch.tensor(input_dec)
  
      input_enc = input_enc.cuda()
      input_dec = input_dec.cuda()
      
      outputs = model(input_ids=input_enc, decoder_input_ids=input_dec)
      logits = outputs.logits # only compute loss once
      loss = objective(logits.view(-1, tokenizer.get_vocab_size()), input_enc.view(-1)) # ignore padding in loss function
  
      loss.backward()
      loop.update(1)
  
      if batch % 4 == 0:
        if batch % 24 == 0:
          loop.set_description('{:.3f}'.format(loss.item()))
        if batch % 1000 == 0:
          generated = model.generate(input_enc[0].unsqueeze(0))
          print(sent[0][0])
          print(generated[0].cpu().numpy())
          print(tokenizer.decode(generated[0].cpu().numpy(),skip_special_tokens=True))
        optimizer.step()
        optimizer.zero_grad()
  
      sys.stdout.flush()
    loop.close()

    torch.save(model.state_dict(), 'best')
scope()
